{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 2100/4700: Introduction to Machine Learning -- Feature Engineering\n",
    "**Goal:** Transform raw data into powerful predictive features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Feature Engineering is the \"Art\" of Machine Learning\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering.\" — Andrew Ng\n",
    "\n",
    "### The Feature Engineering Impact\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    THE FEATURE ENGINEERING MULTIPLIER                   │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│   RAW DATA          GOOD FEATURES         GREAT MODEL                   │\n",
    "│   ──────────   +    ────────────    =     ───────────                   │\n",
    "│                                                                         │\n",
    "│   Same algorithm + better features >>> better algorithm + poor features │\n",
    "│                                                                         │\n",
    "│   Example:                                                              │\n",
    "│   • Raw: birth_date = \"1985-03-15\"                                      │\n",
    "│   • Feature: age = 39                                                   │\n",
    "│   • Better: age_group = \"35-44\"                                         │\n",
    "│   • Even better: is_working_age = 1, years_to_retirement = 26           │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### What is Feature Engineering?\n",
    "\n",
    "| Term | Definition | Example |\n",
    "|------|------------|--------|\n",
    "| **Feature** | An individual measurable property used as input to a model | `age`, `income`, `hours_worked` |\n",
    "| **Feature Engineering** | The process of creating new features from existing data | `age` → `age_squared`, `is_senior` |\n",
    "| **Feature Extraction** | Deriving features from raw data | `timestamp` → `hour`, `day_of_week` |\n",
    "| **Feature Selection** | Choosing the most relevant features | Keep `age`, drop `ssn` |\n",
    "| **Feature Transformation** | Changing feature representation | `income` → `log(income)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Feature Engineering Matters: Real Examples\n",
    "\n",
    "| Scenario | Raw Feature | Engineered Feature | Model Improvement |\n",
    "|----------|-------------|-------------------|-------------------|\n",
    "| Predicting house prices | `year_built = 1995` | `house_age = 29` | +15% accuracy |\n",
    "| Fraud detection | `transaction_time = 3:00 AM` | `is_unusual_hour = 1` | +25% recall |\n",
    "| Customer churn | `last_purchase = 2024-01-15` | `days_since_purchase = 45` | +20% precision |\n",
    "| Salary prediction | `education = \"PhD\"` | `education_years = 20` | +10% R² |\n",
    "\n",
    "### The Feature Engineering Mindset\n",
    "\n",
    "Think like a domain expert! Ask yourself:\n",
    "\n",
    "| Question | Feature Ideas |\n",
    "|----------|---------------|\n",
    "| What would a human expert look at? | Domain-specific ratios, thresholds |\n",
    "| What patterns exist in the data? | Trends, seasonality, clusters |\n",
    "| What combinations might matter? | Interactions, ratios, differences |\n",
    "| What external knowledge applies? | Industry benchmarks, known thresholds |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "1. Extract features from datetime variables\n",
    "2. Apply multiple categorical encoding strategies\n",
    "3. Create numerical transformations and interactions\n",
    "4. Perform feature selection using multiple methods\n",
    "5. Build a complete feature engineering pipeline\n",
    "6. Avoid common feature engineering pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set visual style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Load data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "           'marital_status', 'occupation', 'relationship', 'race',\n",
    "           'sex', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "           'native_country', 'income']\n",
    "df = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
    "\n",
    "# Clean\n",
    "df = df.dropna()\n",
    "df['income_binary'] = (df['income'] == '>50K').astype(int)\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns available for feature engineering:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DateTime Feature Engineering\n",
    "\n",
    "### 1.1 Why DateTime Features Matter\n",
    "\n",
    "DateTime is one of the richest sources of features! A single timestamp can generate dozens of useful features.\n",
    "\n",
    "| DateTime Component | What It Captures | Example Use Case |\n",
    "|--------------------|------------------|------------------|\n",
    "| **Year** | Long-term trends | Housing prices over decades |\n",
    "| **Month** | Seasonality | Retail sales patterns |\n",
    "| **Day of Month** | Pay cycles | Spending behavior |\n",
    "| **Day of Week** | Weekly patterns | Restaurant traffic |\n",
    "| **Hour** | Daily patterns | Energy consumption |\n",
    "| **Is Weekend** | Work vs leisure | Website traffic |\n",
    "| **Quarter** | Business cycles | Financial reporting |\n",
    "| **Is Holiday** | Special events | Travel demand |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basic DateTime Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample datetime column for demonstration\n",
    "# (Adult dataset doesn't have dates, so we'll create one)\n",
    "np.random.seed(42)\n",
    "df['application_date'] = pd.date_range(\n",
    "    start='2020-01-01', \n",
    "    periods=len(df), \n",
    "    freq='H'\n",
    ")[:len(df)]\n",
    "\n",
    "print(\"Sample datetime values:\")\n",
    "print(df['application_date'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Extracting DateTime Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df, date_col):\n",
    "    \"\"\"\n",
    "    Extract comprehensive datetime features from a date column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "    date_col : str - name of datetime column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with new datetime features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure datetime type\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Basic extractions\n",
    "    df[f'{date_col}_year'] = df[date_col].dt.year\n",
    "    df[f'{date_col}_month'] = df[date_col].dt.month\n",
    "    df[f'{date_col}_day'] = df[date_col].dt.day\n",
    "    df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek  # 0=Monday\n",
    "    df[f'{date_col}_hour'] = df[date_col].dt.hour\n",
    "    df[f'{date_col}_quarter'] = df[date_col].dt.quarter\n",
    "    \n",
    "    # Derived features\n",
    "    df[f'{date_col}_is_weekend'] = df[date_col].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    df[f'{date_col}_is_month_start'] = df[date_col].dt.is_month_start.astype(int)\n",
    "    df[f'{date_col}_is_month_end'] = df[date_col].dt.is_month_end.astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply datetime extraction\n",
    "df = extract_datetime_features(df, 'application_date')\n",
    "\n",
    "# Show new features\n",
    "datetime_cols = [col for col in df.columns if 'application_date_' in col]\n",
    "print(\"Extracted DateTime Features:\")\n",
    "print(df[datetime_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Cyclical Encoding for DateTime\n",
    "\n",
    "**Problem:** Month 12 (December) and Month 1 (January) are numerically far apart (12 vs 1) but actually adjacent in time!\n",
    "\n",
    "**Solution:** Use sine and cosine transformations to capture cyclical nature.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    CYCLICAL ENCODING VISUALIZATION                      │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│   Linear Encoding:     1 ─── 2 ─── 3 ─── ... ─── 11 ─── 12              │\n",
    "│                        ↑                                   ↑            │\n",
    "│                        └───────── FAR APART ───────────────┘            │\n",
    "│                                                                         │\n",
    "│   Cyclical Encoding:          12  1                                     │\n",
    "│                             11      2                                   │\n",
    "│                           10          3                                 │\n",
    "│                             9          4                                │\n",
    "│                              8        5                                 │\n",
    "│                                 7  6                                    │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encode(df, col, max_val):\n",
    "    \"\"\"\n",
    "    Apply cyclical encoding using sin/cos transformation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "    col : str - column to encode\n",
    "    max_val : int - maximum value in the cycle (e.g., 12 for months, 7 for days)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with sin and cos encoded columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "# Apply cyclical encoding\n",
    "df = cyclical_encode(df, 'application_date_month', 12)\n",
    "df = cyclical_encode(df, 'application_date_dayofweek', 7)\n",
    "df = cyclical_encode(df, 'application_date_hour', 24)\n",
    "\n",
    "print(\"Cyclical Encoding Example (Month):\")\n",
    "print(df[['application_date_month', 'application_date_month_sin', \n",
    "          'application_date_month_cos']].drop_duplicates().sort_values(\n",
    "          'application_date_month').head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Visual: Cyclical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cyclical encoding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Month on a circle\n",
    "months = np.arange(1, 13)\n",
    "month_sin = np.sin(2 * np.pi * months / 12)\n",
    "month_cos = np.cos(2 * np.pi * months / 12)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(month_cos, month_sin, s=200, c=months, cmap='hsv')\n",
    "for i, m in enumerate(months):\n",
    "    ax.annotate(f'M{m}', (month_cos[i]+0.1, month_sin[i]+0.1), fontsize=12)\n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.set_xlabel('Cosine')\n",
    "ax.set_ylabel('Sine')\n",
    "ax.set_title('Months on Unit Circle\\n(Adjacent months are close!)', fontsize=14, fontweight='bold')\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Plot 2: Comparison of encodings\n",
    "ax = axes[1]\n",
    "ax.plot(months, months, 'o-', label='Linear (1-12)', linewidth=2)\n",
    "ax.plot(months, month_sin, 's-', label='Sin encoding', linewidth=2)\n",
    "ax.plot(months, month_cos, '^-', label='Cos encoding', linewidth=2)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Encoded Value')\n",
    "ax.set_title('Linear vs Cyclical Encoding', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xticks(months)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: DateTime Feature Engineering\n",
    "\n",
    "Create datetime features from a purchase timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given this sample data\n",
    "sample_dates = pd.DataFrame({\n",
    "    'purchase_time': pd.to_datetime([\n",
    "        '2024-12-25 14:30:00',  # Christmas afternoon\n",
    "        '2024-07-04 09:00:00',  # July 4th morning\n",
    "        '2024-01-01 00:15:00',  # New Year's midnight\n",
    "        '2024-03-15 18:45:00',  # Random weekday evening\n",
    "        '2024-06-15 12:00:00',  # Saturday noon\n",
    "    ])\n",
    "})\n",
    "\n",
    "# Task 1: Extract year, month, day, hour, dayofweek\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Task 2: Create is_weekend feature\n",
    "\n",
    "\n",
    "# Task 3: Create is_business_hours feature (9 AM - 5 PM on weekdays)\n",
    "\n",
    "\n",
    "# Task 4: Apply cyclical encoding to hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Task 1: Basic extraction\n",
    "sample_dates['year'] = sample_dates['purchase_time'].dt.year\n",
    "sample_dates['month'] = sample_dates['purchase_time'].dt.month\n",
    "sample_dates['day'] = sample_dates['purchase_time'].dt.day\n",
    "sample_dates['hour'] = sample_dates['purchase_time'].dt.hour\n",
    "sample_dates['dayofweek'] = sample_dates['purchase_time'].dt.dayofweek\n",
    "\n",
    "# Task 2: Is weekend\n",
    "sample_dates['is_weekend'] = sample_dates['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Task 3: Is business hours\n",
    "sample_dates['is_business_hours'] = (\n",
    "    (sample_dates['hour'] >= 9) & \n",
    "    (sample_dates['hour'] < 17) & \n",
    "    (sample_dates['dayofweek'] < 5)\n",
    ").astype(int)\n",
    "\n",
    "# Task 4: Cyclical encoding for hour\n",
    "sample_dates['hour_sin'] = np.sin(2 * np.pi * sample_dates['hour'] / 24)\n",
    "sample_dates['hour_cos'] = np.cos(2 * np.pi * sample_dates['hour'] / 24)\n",
    "\n",
    "print(sample_dates)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Categorical Feature Engineering\n",
    "\n",
    "### 2.1 Encoding Methods Overview\n",
    "\n",
    "| Method | When to Use | Pros | Cons |\n",
    "|--------|-------------|------|------|\n",
    "| **Label Encoding** | Ordinal categories | Simple, memory efficient | Implies false ordering |\n",
    "| **One-Hot Encoding** | Nominal categories, low cardinality | No ordering implied | High dimensionality |\n",
    "| **Target Encoding** | High cardinality | Captures target relationship | Risk of data leakage |\n",
    "| **Frequency Encoding** | High cardinality | Simple, no leakage | Loses category identity |\n",
    "| **Binary Encoding** | High cardinality | Compact representation | Less interpretable |\n",
    "\n",
    "### 2.2 Encoding Decision Guide\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────────┐\n",
    "                    │  Is there a natural     │\n",
    "                    │  ORDER to categories?   │\n",
    "                    └───────────┬─────────────┘\n",
    "                                │\n",
    "                    ┌───────────┴───────────┐\n",
    "                    │                       │\n",
    "                   YES                      NO\n",
    "                    │                       │\n",
    "                    v                       v\n",
    "            ┌───────────────┐       ┌───────────────────┐\n",
    "            │ LABEL/ORDINAL │       │ How many unique   │\n",
    "            │ ENCODING      │       │ categories?       │\n",
    "            └───────────────┘       └─────────┬─────────┘\n",
    "                                              │\n",
    "                                    ┌─────────┴─────────┐\n",
    "                                    │                   │\n",
    "                                  < 10               >= 10\n",
    "                                    │                   │\n",
    "                                    v                   v\n",
    "                            ┌───────────────┐   ┌───────────────┐\n",
    "                            │ ONE-HOT       │   │ TARGET or     │\n",
    "                            │ ENCODING      │   │ FREQUENCY     │\n",
    "                            └───────────────┘   │ ENCODING      │\n",
    "                                                └───────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Label Encoding (for Ordinal Data)\n",
    "\n",
    "Use when categories have a meaningful order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education has a natural order\n",
    "education_order = {\n",
    "    'Preschool': 1, '1st-4th': 2, '5th-6th': 3, '7th-8th': 4,\n",
    "    '9th': 5, '10th': 6, '11th': 7, '12th': 8, 'HS-grad': 9,\n",
    "    'Some-college': 10, 'Assoc-voc': 11, 'Assoc-acdm': 12,\n",
    "    'Bachelors': 13, 'Masters': 14, 'Prof-school': 15, 'Doctorate': 16\n",
    "}\n",
    "\n",
    "df['education_ordinal'] = df['education'].map(education_order)\n",
    "\n",
    "print(\"Education Ordinal Encoding:\")\n",
    "print(df[['education', 'education_ordinal']].drop_duplicates().sort_values('education_ordinal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 One-Hot Encoding (for Nominal Data)\n",
    "\n",
    "Use when categories have no natural order and cardinality is low (< 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'sex' (only 2 categories)\n",
    "sex_dummies = pd.get_dummies(df['sex'], prefix='sex', drop_first=True)\n",
    "print(\"One-Hot Encoding (sex):\")\n",
    "print(sex_dummies.head())\n",
    "print(f\"New columns: {sex_dummies.columns.tolist()}\")\n",
    "\n",
    "# One-hot encode 'workclass' (7 categories - manageable)\n",
    "workclass_dummies = pd.get_dummies(df['workclass'], prefix='workclass')\n",
    "print(f\"\\nOne-Hot Encoding (workclass):\")\n",
    "print(f\"Original: 1 column → New: {len(workclass_dummies.columns)} columns\")\n",
    "print(f\"Columns: {workclass_dummies.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Target Encoding (for High Cardinality)\n",
    "\n",
    "Replace category with average target value for that category.\n",
    "\n",
    "**WARNING:** Must be done carefully to avoid data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(df, cat_col, target_col, smoothing=10):\n",
    "    \"\"\"\n",
    "    Apply target encoding with smoothing to prevent overfitting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "    cat_col : str - categorical column to encode\n",
    "    target_col : str - target column\n",
    "    smoothing : int - smoothing factor (higher = more regularization)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Series with target-encoded values\n",
    "    \"\"\"\n",
    "    # Global mean\n",
    "    global_mean = df[target_col].mean()\n",
    "    \n",
    "    # Category statistics\n",
    "    cat_stats = df.groupby(cat_col)[target_col].agg(['mean', 'count'])\n",
    "    \n",
    "    # Smoothed target encoding\n",
    "    # Formula: (category_mean * count + global_mean * smoothing) / (count + smoothing)\n",
    "    smooth_mean = (cat_stats['mean'] * cat_stats['count'] + global_mean * smoothing) / (cat_stats['count'] + smoothing)\n",
    "    \n",
    "    return df[cat_col].map(smooth_mean)\n",
    "\n",
    "# Apply target encoding to 'occupation' (14 categories - high cardinality)\n",
    "df['occupation_target_encoded'] = target_encode(df, 'occupation', 'income_binary')\n",
    "\n",
    "print(\"Target Encoding (occupation):\")\n",
    "print(df.groupby('occupation').agg({\n",
    "    'income_binary': 'mean',\n",
    "    'occupation_target_encoded': 'first'\n",
    "}).sort_values('income_binary', ascending=False).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Frequency Encoding\n",
    "\n",
    "Replace category with its frequency in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encode(df, col):\n",
    "    \"\"\"\n",
    "    Apply frequency encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "    col : str - column to encode\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Series with frequency-encoded values\n",
    "    \"\"\"\n",
    "    freq = df[col].value_counts(normalize=True)\n",
    "    return df[col].map(freq)\n",
    "\n",
    "# Apply to native_country (41 categories - very high cardinality)\n",
    "df['native_country_freq'] = frequency_encode(df, 'native_country')\n",
    "\n",
    "print(\"Frequency Encoding (native_country):\")\n",
    "print(df[['native_country', 'native_country_freq']].drop_duplicates().sort_values(\n",
    "    'native_country_freq', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Comparison of Encoding Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare encoding methods for 'occupation'\n",
    "print(\"=\"*70)\n",
    "print(\"ENCODING COMPARISON: occupation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Original\n",
    "print(f\"\\nOriginal unique values: {df['occupation'].nunique()}\")\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "df['occupation_label'] = le.fit_transform(df['occupation'])\n",
    "\n",
    "# One-hot would create 14 columns (too many to show)\n",
    "occupation_onehot = pd.get_dummies(df['occupation'], prefix='occ')\n",
    "\n",
    "# Target encoding (already done)\n",
    "# Frequency encoding\n",
    "df['occupation_freq'] = frequency_encode(df, 'occupation')\n",
    "\n",
    "print(\"\\nSample comparison:\")\n",
    "comparison = df[['occupation', 'occupation_label', 'occupation_target_encoded', \n",
    "                 'occupation_freq']].drop_duplicates().head(5)\n",
    "comparison.columns = ['Original', 'Label', 'Target', 'Frequency']\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(f\"\\nOne-Hot would create: {len(occupation_onehot.columns)} new columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Categorical Encoding\n",
    "\n",
    "Apply different encoding methods to the `marital_status` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: How many unique values does marital_status have?\n",
    "n_unique = _____\n",
    "print(f\"Unique values: {n_unique}\")\n",
    "\n",
    "# Task 2: Apply one-hot encoding\n",
    "marital_onehot = _____\n",
    "print(f\"One-hot columns: {marital_onehot.shape[1]}\")\n",
    "\n",
    "# Task 3: Apply frequency encoding\n",
    "df['marital_freq'] = _____\n",
    "\n",
    "# Task 4: Apply target encoding\n",
    "df['marital_target'] = _____\n",
    "\n",
    "# Task 5: Which marital status has the highest income rate?\n",
    "# (Use target encoding to find out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Task 1\n",
    "n_unique = df['marital_status'].nunique()\n",
    "print(f\"Unique values: {n_unique}\")  # 7\n",
    "\n",
    "# Task 2\n",
    "marital_onehot = pd.get_dummies(df['marital_status'], prefix='marital')\n",
    "print(f\"One-hot columns: {marital_onehot.shape[1]}\")  # 7\n",
    "\n",
    "# Task 3\n",
    "df['marital_freq'] = frequency_encode(df, 'marital_status')\n",
    "\n",
    "# Task 4\n",
    "df['marital_target'] = target_encode(df, 'marital_status', 'income_binary')\n",
    "\n",
    "# Task 5\n",
    "print(\"\\nMarital status by income rate:\")\n",
    "print(df.groupby('marital_status')['income_binary'].mean().sort_values(ascending=False))\n",
    "# Married-civ-spouse has highest income rate\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Choosing the Right Encoding\n",
    "\n",
    "For each column, decide which encoding method to use and explain why:\n",
    "\n",
    "| Column | Unique Values | Has Order? | Your Choice | Why? |\n",
    "|--------|---------------|------------|-------------|------|\n",
    "| `sex` | 2 | No | _____ | _____ |\n",
    "| `education` | 16 | Yes | _____ | _____ |\n",
    "| `native_country` | 41 | No | _____ | _____ |\n",
    "| `race` | 5 | No | _____ | _____ |\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answers</summary>\n",
    "\n",
    "| Column | Your Choice | Why? |\n",
    "|--------|-------------|------|\n",
    "| `sex` | One-Hot (or binary) | Only 2 categories, no order |\n",
    "| `education` | Label/Ordinal | Has natural order (education level) |\n",
    "| `native_country` | Frequency or Target | High cardinality (41), one-hot creates too many columns |\n",
    "| `race` | One-Hot | Low cardinality (5), no order |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical Feature Engineering\n",
    "\n",
    "### 3.1 Types of Numerical Transformations\n",
    "\n",
    "| Transformation | When to Use | Example |\n",
    "|----------------|-------------|--------|\n",
    "| **Binning** | Create categories from continuous | Age → Age Groups |\n",
    "| **Log Transform** | Reduce skewness | Income → Log(Income) |\n",
    "| **Square/Cube** | Capture non-linear relationships | Age → Age² |\n",
    "| **Interaction** | Capture combined effects | Hours × Education |\n",
    "| **Ratio** | Normalize by another feature | Income / Hours = Hourly Rate |\n",
    "| **Binary Flag** | Indicate presence/absence | Capital Gain > 0 |\n",
    "| **Clipping** | Handle outliers | Cap at 99th percentile |\n",
    "| **Polynomial** | Capture curves | Age, Age², Age³ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Binning: Converting Continuous to Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Equal-width bins\n",
    "df['age_bins_equal'] = pd.cut(df['age'], bins=5, labels=['Very Young', 'Young', 'Middle', 'Senior', 'Elder'])\n",
    "\n",
    "# Method 2: Custom bins (domain knowledge)\n",
    "age_bins = [0, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = ['<25', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Method 3: Quantile bins (equal frequency)\n",
    "df['age_quantile'] = pd.qcut(df['age'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "\n",
    "print(\"Binning Comparison:\")\n",
    "print(df[['age', 'age_bins_equal', 'age_group', 'age_quantile']].head(10))\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nAge Group Distribution:\")\n",
    "print(df['age_group'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 When to Use Each Binning Method\n",
    "\n",
    "| Method | When to Use | Example |\n",
    "|--------|-------------|--------|\n",
    "| **Equal-width** | Data is uniformly distributed | Temperature ranges |\n",
    "| **Custom bins** | Domain knowledge exists | Age groups, income brackets |\n",
    "| **Quantile bins** | Data is skewed, want equal samples per bin | Customer segments |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform highly skewed features\n",
    "df['capital_gain_log'] = np.log1p(df['capital_gain'])  # log1p handles zeros\n",
    "df['capital_loss_log'] = np.log1p(df['capital_loss'])\n",
    "\n",
    "print(\"Log Transform Effect:\")\n",
    "print(f\"capital_gain - Original skew: {df['capital_gain'].skew():.2f}\")\n",
    "print(f\"capital_gain - After log:     {df['capital_gain_log'].skew():.2f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['capital_gain'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Original capital_gain\\n(Highly Skewed)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Value')\n",
    "\n",
    "axes[1].hist(df['capital_gain_log'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_title('Log-transformed capital_gain\\n(Less Skewed)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Log(Value + 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features for age\n",
    "df['age_squared'] = df['age'] ** 2\n",
    "df['age_cubed'] = df['age'] ** 3\n",
    "\n",
    "# Why? Income often increases with age, peaks, then decreases\n",
    "# A quadratic term can capture this curve!\n",
    "\n",
    "print(\"Polynomial Features (age):\")\n",
    "print(df[['age', 'age_squared', 'age_cubed']].describe().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Interaction Features\n",
    "\n",
    "Interaction features capture the COMBINED effect of two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "df['age_x_education'] = df['age'] * df['education_num']\n",
    "df['age_x_hours'] = df['age'] * df['hours_per_week']\n",
    "df['education_x_hours'] = df['education_num'] * df['hours_per_week']\n",
    "\n",
    "print(\"Interaction Features:\")\n",
    "print(df[['age', 'education_num', 'hours_per_week', \n",
    "          'age_x_education', 'age_x_hours', 'education_x_hours']].head())\n",
    "\n",
    "# Why interactions matter:\n",
    "# High education ALONE doesn't guarantee high income\n",
    "# Many hours ALONE doesn't guarantee high income\n",
    "# But high education AND many hours together often does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Ratio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ratio features\n",
    "df['hourly_rate_proxy'] = df['capital_gain'] / (df['hours_per_week'] * 52 + 1)  # +1 to avoid division by zero\n",
    "df['capital_net'] = df['capital_gain'] - df['capital_loss']\n",
    "df['capital_ratio'] = df['capital_gain'] / (df['capital_loss'] + 1)\n",
    "\n",
    "print(\"Ratio Features:\")\n",
    "print(df[['capital_gain', 'capital_loss', 'capital_net', 'capital_ratio']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Binary Flag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary flag features\n",
    "df['has_capital_gain'] = (df['capital_gain'] > 0).astype(int)\n",
    "df['has_capital_loss'] = (df['capital_loss'] > 0).astype(int)\n",
    "df['works_overtime'] = (df['hours_per_week'] > 40).astype(int)\n",
    "df['is_senior'] = (df['age'] >= 60).astype(int)\n",
    "df['is_highly_educated'] = (df['education_num'] >= 13).astype(int)  # Bachelor's or higher\n",
    "\n",
    "print(\"Binary Flag Features:\")\n",
    "print(df[['has_capital_gain', 'has_capital_loss', 'works_overtime', \n",
    "          'is_senior', 'is_highly_educated']].sum())\n",
    "print(\"\\nPercentages:\")\n",
    "print((df[['has_capital_gain', 'has_capital_loss', 'works_overtime', \n",
    "           'is_senior', 'is_highly_educated']].mean() * 100).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Numerical Feature Engineering\n",
    "\n",
    "Create the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Create 'hours_per_week_squared'\n",
    "df['hours_squared'] = _____\n",
    "\n",
    "# Task 2: Create 'age_hours_interaction'\n",
    "df['age_hours'] = _____\n",
    "\n",
    "# Task 3: Create binary flag 'works_fulltime' (>= 35 hours)\n",
    "df['works_fulltime'] = _____\n",
    "\n",
    "# Task 4: Create age bins: 'Young' (<30), 'Middle' (30-50), 'Senior' (>50)\n",
    "df['age_category'] = _____\n",
    "\n",
    "# Task 5: Create log transform of (hours_per_week + 1)\n",
    "df['hours_log'] = _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Task 1\n",
    "df['hours_squared'] = df['hours_per_week'] ** 2\n",
    "\n",
    "# Task 2\n",
    "df['age_hours'] = df['age'] * df['hours_per_week']\n",
    "\n",
    "# Task 3\n",
    "df['works_fulltime'] = (df['hours_per_week'] >= 35).astype(int)\n",
    "\n",
    "# Task 4\n",
    "df['age_category'] = pd.cut(df['age'], \n",
    "                            bins=[0, 30, 50, 100], \n",
    "                            labels=['Young', 'Middle', 'Senior'])\n",
    "\n",
    "# Task 5\n",
    "df['hours_log'] = np.log1p(df['hours_per_week'])\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(df[['hours_per_week', 'hours_squared', 'age_hours', \n",
    "          'works_fulltime', 'age_category', 'hours_log']].head())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Feature Engineering Creativity\n",
    "\n",
    "Come up with 3 NEW feature ideas for the Adult dataset that we haven't discussed. Explain why each might be useful:\n",
    "\n",
    "| Feature Name | Formula/Logic | Why It Might Be Useful |\n",
    "|--------------|---------------|------------------------|\n",
    "| ____________ | ____________ | ____________ |\n",
    "| ____________ | ____________ | ____________ |\n",
    "| ____________ | ____________ | ____________ |\n",
    "\n",
    "<details>\n",
    "<summary>Click for Example Answers</summary>\n",
    "\n",
    "| Feature Name | Formula/Logic | Why It Might Be Useful |\n",
    "|--------------|---------------|------------------------|\n",
    "| `years_until_retirement` | `65 - age` | Captures career stage, may correlate with savings behavior |\n",
    "| `education_per_age` | `education_num / age` | Early achievers vs late bloomers |\n",
    "| `is_married` | `marital_status.contains('Married')` | Simpler binary version of marital status |\n",
    "| `capital_active` | `has_capital_gain OR has_capital_loss` | Shows investment activity |\n",
    "| `work_intensity` | `hours_per_week / 40` | Normalized work effort |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Selection\n",
    "\n",
    "### 4.1 Why Feature Selection Matters\n",
    "\n",
    "| Problem with Too Many Features | Consequence |\n",
    "|-------------------------------|-------------|\n",
    "| **Curse of dimensionality** | Need exponentially more data |\n",
    "| **Overfitting** | Model learns noise, not signal |\n",
    "| **Slow training** | More features = more computation |\n",
    "| **Harder to interpret** | Can't explain 500 features |\n",
    "| **Multicollinearity** | Correlated features confuse models |\n",
    "\n",
    "### 4.2 Feature Selection Methods\n",
    "\n",
    "| Method | Type | How It Works |\n",
    "|--------|------|-------------|\n",
    "| **Correlation** | Filter | Remove highly correlated features |\n",
    "| **Variance Threshold** | Filter | Remove low-variance features |\n",
    "| **Mutual Information** | Filter | Keep features with high info about target |\n",
    "| **Chi-Square** | Filter | For categorical features vs categorical target |\n",
    "| **Feature Importance** | Embedded | Use tree models to rank importance |\n",
    "| **Recursive Feature Elimination** | Wrapper | Iteratively remove weakest features |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Method 1: Correlation-Based Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated feature pairs\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove target and ID-like columns\n",
    "numeric_features = [c for c in numeric_features if c not in ['income_binary', 'fnlwgt']]\n",
    "\n",
    "corr_matrix = df[numeric_features].corr().abs()\n",
    "\n",
    "# Find pairs with correlation > 0.8\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': corr_matrix.columns[i],\n",
    "                'Feature 2': corr_matrix.columns[j],\n",
    "                'Correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "print(\"Highly Correlated Feature Pairs (>0.8):\")\n",
    "if high_corr_pairs:\n",
    "    print(pd.DataFrame(high_corr_pairs))\n",
    "else:\n",
    "    print(\"No pairs found with correlation > 0.8\")\n",
    "\n",
    "# Rule: When two features are highly correlated, keep only one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Method 2: Feature Importance (Tree-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Random Forest to get feature importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare features (only original numeric + some engineered)\n",
    "feature_cols = ['age', 'education_num', 'capital_gain', 'capital_loss', \n",
    "                'hours_per_week', 'age_squared', 'has_capital_gain']\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['income_binary']\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Method 3: Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate mutual information\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'MI Score': mi_scores\n",
    "}).sort_values('MI Score', ascending=False)\n",
    "\n",
    "print(\"Mutual Information Scores:\")\n",
    "print(mi_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Feature Selection Decision Framework\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                  FEATURE SELECTION DECISION FRAMEWORK                   │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  STEP 1: Remove obviously useless features                              │\n",
    "│  ─────────────────────────────────────────                              │\n",
    "│  • IDs, names, timestamps (unless engineered)                           │\n",
    "│  • Features with >50% missing values                                    │\n",
    "│  • Features with near-zero variance                                     │\n",
    "│                          │                                              │\n",
    "│                          ▼                                              │\n",
    "│  STEP 2: Handle multicollinearity                                       │\n",
    "│  ────────────────────────────────                                       │\n",
    "│  • Remove one from each pair with correlation > 0.9                     │\n",
    "│  • Keep the one more correlated with target                             │\n",
    "│                          │                                              │\n",
    "│                          ▼                                              │\n",
    "│  STEP 3: Rank remaining features                                        │\n",
    "│  ───────────────────────────────                                        │\n",
    "│  • Use feature importance OR mutual information                         │\n",
    "│  • Keep top K features (K based on domain knowledge or CV)              │\n",
    "│                          │                                              │\n",
    "│                          ▼                                              │\n",
    "│  STEP 4: Validate selection                                             │\n",
    "│  ─────────────────────────                                              │\n",
    "│  • Train model with selected features                                   │\n",
    "│  • Compare to model with all features                                   │\n",
    "│  • Check for performance vs complexity tradeoff                         │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given these features and their importance scores:\n",
    "features = {\n",
    "    'age': 0.15,\n",
    "    'education_num': 0.12,\n",
    "    'hours_per_week': 0.10,\n",
    "    'capital_gain': 0.25,\n",
    "    'occupation_encoded': 0.08,\n",
    "    'age_squared': 0.14,  # Highly correlated with age (0.95)\n",
    "    'random_noise': 0.01\n",
    "}\n",
    "\n",
    "# Task 1: Which feature would you remove due to low importance?\n",
    "# Answer: _____\n",
    "\n",
    "# Task 2: Between 'age' and 'age_squared', which would you keep and why?\n",
    "# Answer: _____\n",
    "\n",
    "# Task 3: If you could only keep 4 features, which ones?\n",
    "# Answer: _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Answers</summary>\n",
    "\n",
    "1. **Remove:** `random_noise` (importance = 0.01, essentially useless)\n",
    "\n",
    "2. **Keep `age_squared`** because it has slightly lower importance (0.14 vs 0.15) BUT captures non-linear relationships. Alternatively, keep `age` if interpretability matters more.\n",
    "\n",
    "3. **Top 4 features:** `capital_gain` (0.25), `age` (0.15), `age_squared` (0.14), `education_num` (0.12)\n",
    "   - Or if avoiding correlated features: `capital_gain`, `age`, `education_num`, `hours_per_week`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Feature Engineering Pipeline\n",
    "\n",
    "### 5.1 The Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Complete feature engineering pipeline for Adult dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame - raw data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"FEATURE ENGINEERING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Numerical Transformations\n",
    "    print(\"\\n Step 1: Numerical Transformations\")\n",
    "    df['age_squared'] = df['age'] ** 2\n",
    "    df['age_cubed'] = df['age'] ** 3\n",
    "    df['capital_gain_log'] = np.log1p(df['capital_gain'])\n",
    "    df['capital_loss_log'] = np.log1p(df['capital_loss'])\n",
    "    df['capital_net'] = df['capital_gain'] - df['capital_loss']\n",
    "    print(\"   Created polynomial, log, and arithmetic features\")\n",
    "    \n",
    "    # Step 2: Binary Flags\n",
    "    print(\"\\n Step 2: Binary Flags\")\n",
    "    df['has_capital_gain'] = (df['capital_gain'] > 0).astype(int)\n",
    "    df['has_capital_loss'] = (df['capital_loss'] > 0).astype(int)\n",
    "    df['works_overtime'] = (df['hours_per_week'] > 40).astype(int)\n",
    "    df['is_senior'] = (df['age'] >= 60).astype(int)\n",
    "    df['is_highly_educated'] = (df['education_num'] >= 13).astype(int)\n",
    "    print(\"   Created 5 binary flag features\")\n",
    "    \n",
    "    # Step 3: Binning\n",
    "    print(\"\\n Step 3: Binning\")\n",
    "    age_bins = [0, 25, 35, 45, 55, 65, 100]\n",
    "    age_labels = ['<25', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n",
    "    \n",
    "    hours_bins = [0, 20, 35, 40, 50, 100]\n",
    "    hours_labels = ['Part-time', 'Reduced', 'Full-time', 'Overtime', 'Extreme']\n",
    "    df['hours_group'] = pd.cut(df['hours_per_week'], bins=hours_bins, labels=hours_labels)\n",
    "    print(\"    Created age_group and hours_group bins\")\n",
    "    \n",
    "    # Step 4: Interaction Features\n",
    "    print(\"\\n Step 4: Interaction Features\")\n",
    "    df['age_x_education'] = df['age'] * df['education_num']\n",
    "    df['age_x_hours'] = df['age'] * df['hours_per_week']\n",
    "    df['education_x_hours'] = df['education_num'] * df['hours_per_week']\n",
    "    print(\"   Created 3 interaction features\")\n",
    "    \n",
    "    # Step 5: Categorical Encoding\n",
    "    print(\"\\n Step 5: Categorical Encoding\")\n",
    "    \n",
    "    # Label encode ordinal features\n",
    "    education_order = {\n",
    "        'Preschool': 1, '1st-4th': 2, '5th-6th': 3, '7th-8th': 4,\n",
    "        '9th': 5, '10th': 6, '11th': 7, '12th': 8, 'HS-grad': 9,\n",
    "        'Some-college': 10, 'Assoc-voc': 11, 'Assoc-acdm': 12,\n",
    "        'Bachelors': 13, 'Masters': 14, 'Prof-school': 15, 'Doctorate': 16\n",
    "    }\n",
    "    df['education_ordinal'] = df['education'].map(education_order)\n",
    "    \n",
    "    # Binary encode sex\n",
    "    df['is_male'] = (df['sex'] == 'Male').astype(int)\n",
    "    \n",
    "    # Frequency encode high-cardinality features\n",
    "    for col in ['occupation', 'native_country']:\n",
    "        freq = df[col].value_counts(normalize=True)\n",
    "        df[f'{col}_freq'] = df[col].map(freq)\n",
    "    \n",
    "    print(\"   Encoded education (ordinal), sex (binary), occupation & country (frequency)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    original_cols = 15\n",
    "    new_cols = len(df.columns)\n",
    "    print(f\"Original columns: {original_cols}\")\n",
    "    print(f\"Final columns: {new_cols}\")\n",
    "    print(f\"New features created: {new_cols - original_cols}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the pipeline\n",
    "df_engineered = engineer_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Verify Feature Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_features(df, target_col='income_binary'):\n",
    "    \"\"\"\n",
    "    Verify quality of engineered features.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"FEATURE QUALITY VERIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get numeric columns (excluding target)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != target_col]\n",
    "    \n",
    "    # Check 1: Missing values\n",
    "    print(\"\\n1  Missing Values:\")\n",
    "    missing = df[numeric_cols].isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if len(missing) == 0:\n",
    "        print(\"   No missing values in numeric features\")\n",
    "    else:\n",
    "        print(f\"    Features with missing values: {missing.to_dict()}\")\n",
    "    \n",
    "    # Check 2: Infinite values\n",
    "    print(\"\\n2  Infinite Values:\")\n",
    "    inf_counts = np.isinf(df[numeric_cols]).sum()\n",
    "    inf_counts = inf_counts[inf_counts > 0]\n",
    "    if len(inf_counts) == 0:\n",
    "        print(\"   No infinite values\")\n",
    "    else:\n",
    "        print(f\"    Features with infinite values: {inf_counts.to_dict()}\")\n",
    "    \n",
    "    # Check 3: Constant features\n",
    "    print(\"\\n3  Constant Features (zero variance):\")\n",
    "    constants = [c for c in numeric_cols if df[c].std() == 0]\n",
    "    if len(constants) == 0:\n",
    "        print(\"   No constant features\")\n",
    "    else:\n",
    "        print(f\"   Constant features: {constants}\")\n",
    "    \n",
    "    # Check 4: High correlation with target\n",
    "    print(\"\\n4 Top 5 Features by Target Correlation:\")\n",
    "    correlations = df[numeric_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "    print(correlations.head().to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Verify\n",
    "verify_features(df_engineered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Common Pitfalls and Best Practices\n",
    "\n",
    "### 6.1 The Deadly Sin: Data Leakage\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                          ⚠️  DATA LEAKAGE ⚠️                           │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  Data leakage = Using information during training that won't be         │\n",
    "│                 available at prediction time                            │\n",
    "│                                                                         │\n",
    "│  COMMON CAUSES:                                                         │\n",
    "│  • Target encoding on full data (not just training)                     │\n",
    "│  • Scaling on full data (not just training)                             │\n",
    "│  • Features derived from future data                                    │\n",
    "│  • Including proxy features for target                                  │\n",
    "│                                                                         │\n",
    "│  SYMPTOMS:                                                              │\n",
    "│  • Suspiciously high validation accuracy                                │\n",
    "│  • Model performs poorly in production                                  │\n",
    "│                                                                         │\n",
    "│  PREVENTION:                                                            │\n",
    "│  • Always split data BEFORE feature engineering                         │\n",
    "│  • Fit transformers on training data only                               │\n",
    "│  • Apply same transformers to test/validation data                      │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Correct vs Incorrect Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Leakage! Fitting on all data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Wrong way\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # ❌ Fitting on ALL data\n",
    "X_train, X_test = train_test_split(X_scaled)  # ❌ Then splitting\n",
    "\n",
    "# CORRECT: Fit on training only\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # ✅ Fit on training\n",
    "X_test_scaled = scaler.transform(X_test)  # ✅ Only transform test\n",
    "\n",
    "print(\"✅ Correct pipeline: fit_transform on train, transform only on test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Best Practices Summary\n",
    "\n",
    "| Practice | Description | Why It Matters |\n",
    "|----------|-------------|----------------|\n",
    "| **Split first** | Split data before any transformation | Prevents leakage |\n",
    "| **Fit on train** | Fit encoders/scalers on training only | Simulates production |\n",
    "| **Document features** | Keep track of what you created | Reproducibility |\n",
    "| **Start simple** | Try simple features first | Easier to debug |\n",
    "| **Validate impact** | Check if new features improve model | Avoid complexity |\n",
    "| **Handle edge cases** | What if a new category appears? | Production robustness |\n",
    "| **Version control** | Save feature engineering code | Reproducibility |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Feature Engineering Checklist\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    FEATURE ENGINEERING CHECKLIST                        │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  BEFORE ENGINEERING:                                                    │\n",
    "│  □ Split data into train/test                                           │\n",
    "│  □ Understand each feature's meaning                                    │\n",
    "│  □ Identify feature types (numeric, categorical, datetime)              │\n",
    "│                                                                         │\n",
    "│  DURING ENGINEERING:                                                    │\n",
    "│  □ Create meaningful transformations                                    │\n",
    "│  □ Handle missing values appropriately                                  │\n",
    "│  □ Encode categorical variables                                         │\n",
    "│  □ Create interaction features (if appropriate)                         │\n",
    "│  □ Document each new feature                                            │\n",
    "│                                                                         │\n",
    "│  AFTER ENGINEERING:                                                     │\n",
    "│  □ Check for missing/infinite values                                    │\n",
    "│  □ Remove constant features                                             │\n",
    "│  □ Check for high correlations                                          │\n",
    "│  □ Validate feature importance                                          │\n",
    "│  □ Test model with new features                                         │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Spot the Leakage\n",
    "\n",
    "Which of the following scenarios have data leakage?\n",
    "\n",
    "1. **Scenario A:** You calculate the mean income for each occupation using the entire dataset, then use this as a feature.\n",
    "\n",
    "2. **Scenario B:** You create a feature `days_since_last_purchase` by subtracting purchase date from today's date.\n",
    "\n",
    "3. **Scenario C:** You split data, then apply StandardScaler fitted on training data to both train and test sets.\n",
    "\n",
    "4. **Scenario D:** You include `customer_lifetime_value` as a feature to predict `will_purchase_again`.\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answers</summary>\n",
    "\n",
    "1. **Scenario A: LEAKAGE ⚠️**\n",
    "   - Mean income includes test data information\n",
    "   - Fix: Calculate mean only from training data\n",
    "\n",
    "2. **Scenario B: NO LEAKAGE ✅**\n",
    "   - Using current date is fine (available at prediction time)\n",
    "   - But be careful with \"days since\" features in time series!\n",
    "\n",
    "3. **Scenario C: NO LEAKAGE ✅**\n",
    "   - This is the correct approach!\n",
    "\n",
    "4. **Scenario D: LEAKAGE ⚠️**\n",
    "   - Lifetime value is calculated AFTER purchases happen\n",
    "   - It includes future information about the customer\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Feature Engineering Quick Reference\n",
    "\n",
    "### DateTime Features\n",
    "```python\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['dayofweek'] = df['date'].dt.dayofweek\n",
    "df['is_weekend'] = df['date'].dt.dayofweek.isin([5,6]).astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "```\n",
    "\n",
    "### Categorical Encoding\n",
    "```python\n",
    "# One-hot\n",
    "pd.get_dummies(df['cat_col'], prefix='cat')\n",
    "\n",
    "# Label/Ordinal\n",
    "df['encoded'] = df['cat_col'].map(order_dict)\n",
    "\n",
    "# Frequency\n",
    "df['freq'] = df['cat_col'].map(df['cat_col'].value_counts(normalize=True))\n",
    "```\n",
    "\n",
    "### Numerical Transformations\n",
    "```python\n",
    "# Log transform\n",
    "df['log_col'] = np.log1p(df['col'])\n",
    "\n",
    "# Polynomial\n",
    "df['col_squared'] = df['col'] ** 2\n",
    "\n",
    "# Binning\n",
    "df['binned'] = pd.cut(df['col'], bins=[0, 25, 50, 100], labels=['Low', 'Med', 'High'])\n",
    "\n",
    "# Interaction\n",
    "df['interaction'] = df['col1'] * df['col2']\n",
    "\n",
    "# Binary flag\n",
    "df['flag'] = (df['col'] > threshold).astype(int)\n",
    "```\n",
    "\n",
    "### Feature Selection\n",
    "```python\n",
    "# Correlation\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Random Forest importance\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "importance = rf.feature_importances_\n",
    "\n",
    "# Mutual information\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = mutual_info_classif(X, y)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
